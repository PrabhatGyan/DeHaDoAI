{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df08583",
   "metadata": {},
   "source": [
    "# NCVPRIPG-2025 - DEHADO Challenege\n",
    "## Creating the summary statistics from the InternVL results\n",
    "## Dev version-3\n",
    "Kernel - ncvpripg_2025_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e82c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from jiwer import wer\n",
    "from jiwer import cer\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageStat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_rows', 500)       # Show up to 500 rows\n",
    "pd.set_option('display.max_columns', 100)    # Show up to 100 columns\n",
    "pd.set_option('display.width', 1000)         # Set display width for better readability\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adeca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for post processing\n",
    "from unidecode import unidecode\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "\n",
    "import unicodedata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e589a15",
   "metadata": {},
   "source": [
    "## Load the Post-Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d822a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from post_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/DATA/gyan/GP/ncvpripg2025/dehado/'\n",
    "\n",
    "PATH_PHASE_1 = os.path.join(BASE_PATH,'DEHADO-AI_TRAINING_DATASET')\n",
    "PATH_PHASE_2 = os.path.join(BASE_PATH,'DEHADO-AI_TRAINING_DATASET_PHASE_II')\n",
    "\n",
    "LABEL_PATH_1 = os.path.join(PATH_PHASE_1,'LABELS_750')\n",
    "LABEL_PATH_2 = os.path.join(PATH_PHASE_2,'LABELS_750')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea785b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_1 = 'output_phase_1'\n",
    "TAG_2 = 'output_phase_2'\n",
    "\n",
    "OUTPUT_PATH_1  = os.path.join(BASE_PATH,TAG_1)\n",
    "OUTPUT_PATH_2  = os.path.join(BASE_PATH,TAG_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee38a58",
   "metadata": {},
   "source": [
    "## Load the Label Data for all the images = GROUND TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de572233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt(PATH):\n",
    "\n",
    "    all_labels = os.listdir(PATH)\n",
    "    print(f'We have a total of {len(all_labels)} Label files.')\n",
    "\n",
    "    df_gt = list()\n",
    "    error_fl = list()\n",
    "\n",
    "    for fl in tqdm(all_labels):\n",
    "        \n",
    "        fl_path = os.path.join(PATH,fl)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Load JSON from a file\n",
    "            with open(fl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "\n",
    "            nm,_ = os.path.splitext(fl)\n",
    "\n",
    "            df['filename'] = nm\n",
    "\n",
    "            df_gt.append(df)\n",
    "        except:\n",
    "            error_fl.append(fl)\n",
    "\n",
    "    print(f'# Error Files in GT = {len(error_fl)}')\n",
    "\n",
    "    df_gt = pd.concat(df_gt, axis = 0)\n",
    "    df_gt.rename({'Field name':'field_name','Field value':'field_value'}, axis=1, inplace = True)\n",
    "    df_gt.columns = map(str.lower, df_gt.columns)\n",
    "\n",
    "    # DROP Bounding-Box Columns as of now -> CHANGE\n",
    "    df_gt.drop(['coordinate'], axis = 1, inplace = True)\n",
    "\n",
    "    # Convert field_name and field_value into lowercase\n",
    "    for col in df_gt.select_dtypes(include='object').columns:\n",
    "        df_gt[col] = df_gt[col].str.lower()\n",
    "\n",
    "    # df_gt['field_name'] = df_gt['field_name'].str.lower()\n",
    "    # df_gt['field_value'] = df_gt['field_value'].str.lower()\n",
    "\n",
    "    return df_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_data():\n",
    "\n",
    "    df_gt_1 = get_gt(PATH = LABEL_PATH_1)\n",
    "    df_gt_1['tag'] = TAG_1\n",
    "\n",
    "    df_gt_2 = get_gt(PATH = LABEL_PATH_2)\n",
    "    df_gt_2['tag'] = TAG_2\n",
    "\n",
    "    df_gt = pd.concat([df_gt_1,df_gt_2], axis=0)\n",
    "    df_gt = df_gt.sort_values(['filename','field_name']).reset_index(drop = True)\n",
    "\n",
    "    return df_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8109df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt = ground_truth_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt.head(1)\n",
    "df_gt.shape\n",
    "df_gt['tag'].value_counts(normalize = True)*100\n",
    "df_gt['filename'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28eae63",
   "metadata": {},
   "source": [
    "## Create a dataframe of all the OCRed Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92838467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(txt):\n",
    "\n",
    "    d = txt.lower()\n",
    "    d = d.replace('json',\"\")\n",
    "    d = d.replace('```',\"\")\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17964bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_output(json_obj):\n",
    "\n",
    "    # Flatten the JSON\n",
    "    records = []\n",
    "\n",
    "    for key, value in json_obj.items():\n",
    "        if key == 'references_mobile_number':\n",
    "            if isinstance(value, list):\n",
    "                if len(value) > 0:\n",
    "                    records.append({'field_name': 'referencescmob1', 'ocr_text': str(value[0])})\n",
    "                if len(value) > 1:\n",
    "                    records.append({'field_name': 'referencescmob2', 'ocr_text': str(value[1])})\n",
    "        elif key == 'experience':\n",
    "            if isinstance(value, list):\n",
    "                if len(value) > 0:\n",
    "                    records.append({'field_name': 'experience', 'ocr_text': str(value[0])})\n",
    "                if len(value) > 1:\n",
    "                    records.append({'field_name': 'experience1', 'ocr_text': str(value[1])})\n",
    "        elif isinstance(value, dict):\n",
    "            for sub_key, sub_value in value.items():\n",
    "                records.append({'field_name': f'{key}_{sub_key}', 'ocr_text': str(sub_value)})\n",
    "        elif isinstance(value, list):\n",
    "            records.append({'field_name': key, 'ocr_text': ', '.join(map(str, value))})\n",
    "        else:\n",
    "            records.append({'field_name': key, 'ocr_text': str(value)})\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_json_flattened = pd.DataFrame(records)\n",
    "\n",
    "    return df_json_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ocr_output(OUTPUT_PATH):\n",
    "\n",
    "    all_txt_files = os.listdir(OUTPUT_PATH)\n",
    "    \n",
    "    df_internvl = []\n",
    "    failed_cases = []\n",
    "\n",
    "    for tf in tqdm(all_txt_files):\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Path of the '.txt' file\n",
    "            tf_pth = os.path.join(OUTPUT_PATH,tf)\n",
    "\n",
    "            # Read the '.txt' file\n",
    "            with open(tf_pth, \"r\", encoding=\"utf-8\") as f:\n",
    "                d = f.read()\n",
    "\n",
    "            d = post_processing(txt = d)\n",
    "\n",
    "            obj = json.loads(d)\n",
    "\n",
    "            df = parse_json_output(json_obj = obj)\n",
    "\n",
    "            nm,_ = os.path.splitext(tf)\n",
    "\n",
    "            df['filename'] = nm\n",
    "\n",
    "            df_internvl.append(df)\n",
    "\n",
    "        except:\n",
    "            failed_cases.append(tf)\n",
    "\n",
    "    df_ocr = pd.concat(df_internvl, axis = 0)\n",
    "\n",
    "    # Convert all string values into lowercase\n",
    "    for col in df_ocr.select_dtypes(include='object').columns:\n",
    "        df_ocr[col] = df_ocr[col].str.lower()\n",
    "\n",
    "    return failed_cases,df_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae118b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ocr_data():\n",
    "    \n",
    "    print(f'OUTPUT_PATH_1: We have {len(os.listdir(OUTPUT_PATH_1))} output files from OCR.')\n",
    "    failed_cases_1,df_ocr_1 = get_ocr_output(OUTPUT_PATH = OUTPUT_PATH_1)\n",
    "\n",
    "    print(f'# OUTPUT_PATH_1 - Failure cases: {len(failed_cases_1)}')\n",
    "\n",
    "    print(f'OUTPUT_PATH_2: We have {len(os.listdir(OUTPUT_PATH_2))} output files from OCR.')\n",
    "    failed_cases_2,df_ocr_2 = get_ocr_output(OUTPUT_PATH = OUTPUT_PATH_2)\n",
    "\n",
    "    print(f'# OUTPUT_PATH_2 - Failure cases: {len(failed_cases_2)}')\n",
    "\n",
    "    df_ocr_1['tag'] = TAG_1\n",
    "    df_ocr_2['tag'] = TAG_2\n",
    "\n",
    "    df_ocr = pd.concat([df_ocr_1,df_ocr_2], axis = 0)\n",
    "\n",
    "    return df_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_ocr = get_ocr_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a954d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_ocr.head(2)\n",
    "df_raw_ocr.shape\n",
    "df_raw_ocr.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d96bf9f",
   "metadata": {},
   "source": [
    "## Handle the Govt-ID Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10bb1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the Govt-ID part from the data\n",
    "df_govid_temp = df_raw_ocr[df_raw_ocr['field_name'].isin(['government_id_type','government_id_value'])].sort_values(['tag','filename']).reset_index(drop = True)\n",
    "    \n",
    "# Remove the Govt ID from the OCR data\n",
    "df_raw_ocr = df_raw_ocr[~df_raw_ocr['field_name'].isin(['government_id_type','government_id_value'])].sort_values(['tag','filename']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a9125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_govid = get_govid_subset(df = df_govid_temp)\n",
    "df_govid[['field_name', 'government_id_value_cleaned']] = df_govid['government_id_value'].apply(\n",
    "    lambda x: pd.Series(clean_government_id(x))\n",
    ")\n",
    "df_govid.rename({'government_id_value':'ocr_text'}, axis=1, inplace = True)\n",
    "df_govid = df_govid[['field_name','ocr_text','filename','tag']]\n",
    "\n",
    "df_govid_aadhar = df_govid.copy()\n",
    "df_govid_aadhar['field_name'] = 'aadhaarcard'\n",
    "\n",
    "df_govid_pan = df_govid.copy()\n",
    "df_govid_pan['field_name'] = 'pancard'\n",
    "\n",
    "df_govid = pd.concat([df_govid_aadhar,df_govid_pan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_govid.head(2)\n",
    "df_govid.shape\n",
    "df_govid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocr = pd.concat([df_raw_ocr,df_govid], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocr.head(2)\n",
    "df_ocr.shape\n",
    "df_ocr.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_field_name_dict = {\n",
    "    'alternate_number' : 'alternateno',\n",
    "    'blood_group' : 'bloodgroup',\n",
    "    'candidate_name' : 'candidatename',\n",
    "    'contact_number' : 'contactnumber',\n",
    "    'date_of_birth' : 'dateofbirth',\n",
    "    'father_or_husband_name' : 'father/husbandname',\n",
    "    'languages_known' : 'languageknown',\n",
    "    'marital_status':'maritalstatus',\n",
    "    'permanent_address':'permanentaddress',\n",
    "    'present_address':'presentaddress'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7bb439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column into the OCR data using the matching field_name\n",
    "df_ocr['field_name'] = df_ocr['field_name'].map(change_field_name_dict).fillna(df_ocr['field_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db545ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocr.head(2)\n",
    "df_ocr.shape\n",
    "df_ocr.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10281384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt.head(2)\n",
    "df_gt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812112c",
   "metadata": {},
   "source": [
    "# Post processing - For all the fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb74ce87",
   "metadata": {},
   "source": [
    "## Apply the blanket post-processing for the OCR Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc9554a",
   "metadata": {},
   "source": [
    "# Write the main for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b83e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the list of all the fileds\n",
    "\n",
    "all_field_name = list(df_gt.field_name.unique())\n",
    "print(f'In Ground Truth we have #field_name: {len(all_field_name)}')\n",
    "all_field_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b663f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_base_cleaning = ['bloodgroup','qualification',\n",
    "                    'candidatename','father/husbandname',\n",
    "                    'contactnumber','alternateno','referencescmob1','referencescmob2',\n",
    "                    'date','dateofbirth']\n",
    "\n",
    "def general_post_processing(df,fl):\n",
    "    \n",
    "    if fl in no_base_cleaning:\n",
    "        df['ocr_text_cleaned'] = df['ocr_text']\n",
    "    else:\n",
    "        df['ocr_text_cleaned'] = df['ocr_text'].apply(clean_ocr_text)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad468d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def field_level_post_processing(df,fl,usage = 'yes'):\n",
    "\n",
    "    if usage == 'no':\n",
    "        df['ocr_post_processed_text'] = df['ocr_text_cleaned']\n",
    "    elif usage == 'clean':\n",
    "        if fl == 'yes':\n",
    "            df['ocr_post_processed_text'] = df['ocr_text_cleaned'].apply(clean_nationality)\n",
    "        elif fl == 'gender':\n",
    "            df['ocr_post_processed_text'] = df['ocr_text_cleaned'].apply(clean_gender)\n",
    "        elif fl == 'maritalstatus':\n",
    "            df['ocr_post_processed_text'] = df['ocr_text_cleaned'].apply(clean_marital_status)\n",
    "        if fl == 'bloodgroup':\n",
    "            df['ocr_post_processed_text'] = df['ocr_text_cleaned'].apply(clean_blood_group)\n",
    "        elif fl == 'qualification':\n",
    "            df['ocr_post_processed_text'] = df['ocr_text_cleaned'].apply(clean_qualification)\n",
    "        elif fl in ['candidatename','father/husbandname']:\n",
    "            df['ocr_post_processed_text'] = df['ocr_text_cleaned'].apply(clean_candidate_name)\n",
    "        elif fl in ['contactnumber','alternateno']:\n",
    "            df['ocr_post_processed_text'] = df['ocr_text_cleaned'].apply(clean_indian_contact_number)\n",
    "        elif fl in ['referencescmob1','referencescmob2']:\n",
    "            df['ocr_post_processed_text'] = df['ocr_text_cleaned'].apply(clean_reference_text)\n",
    "        elif fl in ['date','dateofbirth']:\n",
    "            df['ocr_post_processed_text'] = df['ocr_text_cleaned'].apply(clean_and_parse_date)\n",
    "        else:\n",
    "            df['ocr_post_processed_text'] = df['ocr_text_cleaned']\n",
    "    else:\n",
    "        raise ValueError(f'Usage must be either \"yes\" or \"no\", but got {usage}')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9957c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_fill_na(df,fl):\n",
    "\n",
    "    '''\n",
    "    For every filed we shall fill NA with random choice of the valid options of that field\n",
    "    '''\n",
    "\n",
    "    if fl == 'nationality':\n",
    "        valid_options = valid_nationality\n",
    "    elif fl == 'gender':\n",
    "        valid_options = valid_genders\n",
    "    elif fl == 'maritalstatus':\n",
    "        valid_options = valid_statuses\n",
    "    elif fl == 'bloodgroup':\n",
    "        valid_options = valid_blood_groups\n",
    "    elif fl == 'qualification':\n",
    "        valid_options = valid_qualifications\n",
    "    else:\n",
    "        valid_options  = list()\n",
    "\n",
    "    if len(valid_options) > 0:\n",
    "        df['ocr_post_processed_text'] = df['ocr_post_processed_text'].apply(\n",
    "                lambda x: np.random.choice(valid_options) if pd.isna(x) else x\n",
    "            )\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1855712",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = list()\n",
    "\n",
    "for fl in tqdm(all_field_name):\n",
    "\n",
    "    # Make the subset of GroundTruth and OCR Data\n",
    "    \n",
    "    df_gt_fl = df_gt[df_gt['field_name'] == fl].sort_values(['tag','filename']).reset_index(drop=True)\n",
    "    df_ocr_fl = df_ocr[df_ocr['field_name'] == fl].sort_values(['tag','filename']).reset_index(drop=True)\n",
    "\n",
    "    # Apply the Recommended Post-Processing to both the GroundTruth and OCR Text\n",
    "    df_gt_fl['field_value'] = df_gt_fl['field_value'].apply(recommended_cleaning)\n",
    "    df_ocr_fl['ocr_text'] = df_ocr_fl['ocr_text'].apply(recommended_cleaning)\n",
    "\n",
    "\n",
    "    # Apply General Post-Processing to the OCR Text\n",
    "    df_ocr_fl = general_post_processing(df = df_ocr_fl,\n",
    "                                        fl = fl)\n",
    "    \n",
    "    # Apply Filed Level Post-Processing to the OCR Text\n",
    "    df_ocr_fl = field_level_post_processing(df = df_ocr_fl,\n",
    "                                            fl = fl,\n",
    "                                            usage = 'no')\n",
    "\n",
    "    # Merge GroudTruth - OCR Text\n",
    "    \n",
    "    df_result_fl = pd.merge(left = df_gt_fl, \n",
    "                            right = df_ocr_fl, \n",
    "                            on = ['tag','filename','field_name'], \n",
    "                            how = 'left')\n",
    "\n",
    "    df_result_fl = df_result_fl.sort_values(['tag','filename']).reset_index(drop=True)\n",
    "\n",
    "    # Fill the NA values using the random choices of valid values\n",
    "    df_result_fl = random_fill_na(df = df_result_fl,\n",
    "                                  fl = fl)\n",
    "    \n",
    "    # Fill NA Values with empty string for Sanity\n",
    "    # Fill NaNs with empty strings (or handle them as needed)\n",
    "    df_result_fl['field_value'] = df_result_fl['field_value'].fillna(\"\").astype(str)\n",
    "    df_result_fl['ocr_text'] = df_result_fl['ocr_text'].fillna(\"\").astype(str)\n",
    "    df_result_fl['ocr_post_processed_text'] = df_result_fl['ocr_post_processed_text'].fillna(\"\").astype(str)\n",
    "\n",
    "    df_ocr_fl['ocr_post_processed_text'] = df_ocr_fl['ocr_post_processed_text'].apply(recommended_cleaning)\n",
    "\n",
    "    # Calculate WER\n",
    "    df_result_fl['wer_ocr_text'] = df_result_fl.apply(\n",
    "        lambda row: wer(row['field_value'], row['ocr_text']) if row['field_value'] and row['ocr_text'] else None,\n",
    "        axis=1\n",
    "        )\n",
    "    \n",
    "    df_result_fl['wer_ocr_post_processed_text'] = df_result_fl.apply(\n",
    "        lambda row: wer(row['field_value'], row['ocr_post_processed_text']) if row['field_value'] and row['ocr_post_processed_text'] else None,\n",
    "        axis=1\n",
    "        )\n",
    "\n",
    "    # Calculate CER\n",
    "    df_result_fl['cer_ocr_text'] = df_result_fl.apply(\n",
    "        lambda row: cer(row['field_value'], row['ocr_text']) if row['field_value'] and row['ocr_text'] else None,\n",
    "        axis=1\n",
    "        )\n",
    "    \n",
    "    df_result_fl['cer_ocr_post_processed_text'] = df_result_fl.apply(\n",
    "        lambda row: cer(row['field_value'], row['ocr_post_processed_text']) if row['field_value'] and row['ocr_post_processed_text'] else None,\n",
    "        axis=1\n",
    "        )\n",
    "\n",
    "    # Calculate the \"Text Field Accuracy\"\n",
    "    df_result_fl['field_correct_ocr_text'] = np.where(\n",
    "        df_result_fl['field_value'] == df_result_fl['ocr_text'],\n",
    "        1, 0\n",
    "        )\n",
    "\n",
    "    df_result_fl['field_correct_ocr_post_processed_text'] = np.where(\n",
    "        df_result_fl['field_value'] == df_result_fl['ocr_post_processed_text'],\n",
    "        1, 0\n",
    "        )\n",
    "    \n",
    "    # Calculate the \"Document-Level Accuracy\"\n",
    "\n",
    "    # Save the Output\n",
    "    df_output.append(df_result_fl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8d81a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.concat(df_output,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.head(2)\n",
    "df_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5693df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrices(output_data):\n",
    "\n",
    "    wer_val = round(output_data['wer_ocr_text'].mean(),2)\n",
    "    cer_val = round(output_data['cer_ocr_text'].mean(),2)\n",
    "\n",
    "    # Calculate Text Field Accuracy (TFA) and Document-Level Accuracy (DLA)\n",
    "\n",
    "    df_metric = output_data.groupby(['filename']).agg(sum_tfa = ('field_correct_ocr_text','sum'),\n",
    "                                                      nunq_field_name =('field_name', 'nunique')).reset_index()\n",
    "    \n",
    "    df_metric['dla_ind'] = np.where(\n",
    "        df_metric['sum_tfa'] == df_metric['nunq_field_name'],\n",
    "        1, 0\n",
    "        )\n",
    "    \n",
    "    df_metric['tfa_pct'] = (df_metric['sum_tfa'] / df_metric['nunq_field_name']) * 100\n",
    "    \n",
    "    print(df_metric.head(2))\n",
    "\n",
    "    \n",
    "    tfa_pct = round(df_metric['tfa_pct'].mean(),2)\n",
    "    dla_pct = round((df_metric['dla_ind'].sum() / len(df_metric)) * 100,2)\n",
    "\n",
    "    # print(f'Word Error Rate (WER): {wer_val:.2f} \\n Character Error Rate (CER): {cer_val:.2f} \\n Text Field Accuracy (TFA): {tfa_pct.mean():.2f}% \\n Document-Level Accuracy (DLA): {dla_pct:.2f}%')\n",
    "    \n",
    "    return wer_val,cer_val,tfa_pct,dla_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5fb736",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_val,cer_val,tfa_pct,dla_pct = get_metrices(output_data = df_output)\n",
    "print(f'Word Error Rate (WER): {wer_val:.2f} \\n Character Error Rate (CER): {cer_val:.2f} \\n Text Field Accuracy (TFA): {tfa_pct.mean():.2f}% \\n Document-Level Accuracy (DLA): {dla_pct:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f89b27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncvpripg_2025_summary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
